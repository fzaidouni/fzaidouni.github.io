---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---
One of the most astounding questions I have ever faced is *'How I can think, what I think?'*. At times, it seemed paradoxical that I am asking my conscience to make me understand it better. Reading through the works of many great philosophers helped me understand that there is no direct answer, but there are precise [questions](#some-open-questions-that-i-am-intrigued-about-right-now). The questions that land onto precise answers. Further, the foundations of machine learning taught me that the response propagated back from the question, is the most crucial aspect in modelling a better question. Consequently, I found myself understanding the interplay between perception, algorithms of scale and efficiency, and neural cognition with an aim to move towards better [questions](#some-open-questions-that-i-am-intrigued-about-right-now).

I have been working towards many components of these problems with my supervisors. My [work with Dr. Tapan K. Gandhi](/posts/2018/09/early-development-of-onset-vision) since the past three months, is focussed on understanding the development of early-stage perception by examining onset vision patients. Under this research, we are tracking eye twitches using a [visual tracking architecture](/publication/2018-07-ugf) I had designed under the supervision of Dr. G. S. Walia during my junior year. At my [internship at Institue for Pure and Applied Mathematics, UCLA](/posts/2018/08/rips-the-internship/), our team worked on the pipeline of extracting knowledge from raw-data through computational fact-checking. We also devised a [new fact-checking algorithm](https://www.github.com/himahuja/pcatxcore) at-par with the state-of-the-arts. Some of my research has been [published](/publications/) through reputed Journals and Conferences on topics ranging from architectures in deep learning, computer vision, natural language processing, networks, scalable algorithms etc.

### Recent News:
1. Starting a winter internship at Conduent Labs, Xerox on 'Robust Face Recognition under Adversarial Attack'
1. [Optima](https://optimaml.com/) ML Society is up! Check out the first lecture!
1. Presenting "Computational Fact-Checking through Relational Similarity based Path Mining" at [AMS Contributed Session](https://jointmathematicsmeetings.org/meetings/national/jmm2019/2217_progfull.html?fbclid=IwAR0AccnUi_yuX4UdnTVF-cCFVJ5lNYAdIvzw7TPS81eGXk1pn5PvQjaGyTo#2217:AMSCP33) and [Undergraduate Poster Session](https://www.maa.org/programs-and-communities/member-communities/students/undergraduate-research/maa-undergraduate-student-poster-session) [[Abstract](https://jointmathematicsmeetings.org/amsmtgs/2217_abstracts/1145-90-1881.pdf)]
1. Our [publication](https://www.sciencedirect.com/science/article/pii/S0045790618312084), "Supported matrix factorization using distributed representations for personalised recommendations on twitter" got accepted in Journal of Computer Science and Engineering, Elsevier.
2. Completed the [research internship](/posts/2018/08/rips-the-internship/) at IPAM, UCLA under RIPS-NSF Scholarship. [(Project code)](https://github.com/himahuja/pcatxcore). Report available [upon request](mailto:babahooja@gmail.com).
3. Report on [A Reasoning of the Ponzo Effect in Projective Geometry](/posts/2018/05/a-reasoning-of-the-ponzo-effect/) for Visual Perception and Brain by Prof. Dale Purves.
4. [New Blog](/posts/2018/09/linearity-of-reasoning/): How the movie 'Arrival' breaks *Causal Inference*.
5. [New Blog](/posts/2018/09/parallel-feedback-architecture): A new parallel feedback architecture.

In the current wave of Machine Learning in industry, most of the content available was abstracted for instant applicative use. The lack of quality content on foundational knowledge, motivated me to curate [online lectures](/teaching/) on the core principles in Machine Learning. Eventually, we founded a core Machine Intelligence Society *Optima*, in our University, for open source curation of mathematical foundations required in Machine Learning and its applications.

Some open questions that I am intrigued about right now
--------------------------------------------------------
1. **Dealing with catastrophic forgetting with a new Incremental Learning Framework:** Incremental learning is the process of learning new domains of image classes. The multiple iterations over a single datasets optimises the parameters specifically to the dataset such that if a new class is introduced into the training, it undergoes catastrophic forgetting. A different outlook on the problem can be the use of a new class of neural networks which are independent of class labels, but how? ([Email if interested!](mailto:babahooja@gmail.com))
1. **The processing of stream data as a representation of transitions:** The optic nerve encodes the perceived retinal images and sends it to the neocortex. The neocortex observes data in a stream fashion i.e. data is fed to the neocortex through the sensory input at all times. The brain makes its own internal representation of the observation but when the representation stored in memory, what is actually stored? a representation of the observation, or the representation of the transition of the observations? A representation of the observation might seem obvious to me at first, but a representation based on transitions would be invariant to scale, luminance and rotation making the storage more compact and generalised.
2. **To tackle the problem of hierarchy in artificial neural models:** Please find more information on [this blog](/posts/2018/09/parallel-feedback-architecture).

I am working towards these questions actively! [Email me here](mailto:babahooja@gmail.com) in case these catch your interest!
